#!/bin/bash

############################################
## uncomment to re-run a missed dataset
#
# BIT=crawl-data/CC-MAIN-2022-40/segments/1664030334596.27/robotstxt
# ROBONUM="10"
#
# cd robots
# aws s3 sync s3://commoncrawl/$BIT . --quiet
# gunzip *
# cd ..
#
# find ./robots/ -type f | while read file; do
#   grep -B 1 'WARC-Target-URI' $file 2>/dev/null |grep ':' |cut -d ' ' -f 2 | paste - - -d ' ' >> data/robots$ROBONUM.join
# done
#
# node parseRobots.js data/robots$ROBONUM.join | sort | uniq > data/robots$ROBONUM.country
# grep -i 'au ' data/robots$ROBONUM.country |cut -d ' ' -f 2|sort |uniq > data/robots$ROBONUM.country.au
#
# rm -fR robots/*

############################################
## uncomment to re-build unique domains

# echo '' > data/robots-XX.merged.au
# rm -f data/.merged.au.tmp
#
# find ./data/ -type f -name '*.au' | sort | while read file; do
#     cat $file | sort | uniq > $file.tmp && mv $file.tmp $file
#     comm -23 $file data/robots-XX.merged.au > data/.merged.au.tmp
#     cat data/robots-XX.merged.au >> data/.merged.au.tmp
#     cat data/.merged.au.tmp | sort | uniq > data/robots-XX.merged.au
#     rm -f data/.merged.au.tmp
# done

